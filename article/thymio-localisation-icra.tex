% vim: fileencoding=utf-8:tw=0:noexpandtab:ts=4:sw=4
% -*- coding: utf-8 -*-

\documentclass[letterpaper, 10pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts
\overrideIEEEmargins   

% packages
\usepackage[mathletters]{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{url}
\usepackage{tabularx}

% options
\graphicspath{{figures/}{generated-figures/}}

% commands
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tbl}[1]{{Table}~\ref{fig:#1}}
\newcommand{\Tbl}[1]{{Table}~\ref{fig:#1}}
\newcommand{\sect}[1]{Section~\ref{sec:#1}}
\newcommand{\Sect}[1]{Section~\ref{sec:#1}}
\newcommand{\hypo}[1]{Hypothesis~\ref{hyp:#1}}
\newcommand{\Hypo}[1]{Hypothesis~\ref{hyp:#1}}
\newcommand{\algo}[1]{Algorithm~\ref{algo:#1}}
\newcommand{\Algo}[1]{Algorithm~\ref{algo:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~\ref{eq:#1}}

% title and authors

\title{\LARGE \bf
Localization of inexpensive robots with low-bandwidth sensors
}

\author{Shiling Wang$^{1}$ and Francis Colas$^{2}$ and Ming Liu$^{3}$ and Francesco Mondada$^{4}$ and Stéphane Magnenat$^{5}$% <-this % stops a space
\thanks{$^{1}$Shiling Wang is with ETH Zürich
        {\tt\small shilingwang0621@gmail.com}}%
\thanks{$^{2}$Francis Colas is with INRIA Nancy Grand Est
        {\tt\small francis.colas@inria.fr}}%
\thanks{$^{3}$Ming Liu is with City University of Hong Kong
        {\tt\small mingliu@cityu.edu.hk}}%
\thanks{$^{4}$Francesco Mondada is with Mobots, LSRO, EPFL
        {\tt\small francesco.mondada@epfl.ch}}%
\thanks{$^{5}$Stéphane Magnenat is with Mobots, LSRO, EPFL
        {\tt\small stephane@magnenat.net}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
Recent progress in electronics has allowed the construction of affordable mobile robots.
These open many new opportunities, in particular in the educational context.
However, these robots have little long range sensing abilities, as they are still more expensive than shorter range ones, such as infrared sensors.
This lack has prevented these robots from performing global localization without external aids.
This severly limits their use, for example to create advanced educational activities.
In this paper, we propose a solution to this problem, using only infrared ground sensors, approximate dead-reckoning, and a visual pattern on the ground.
Our approach builds on a recursive Bayesian filter, of which we demonstrate two implementations, a dense Markov Localization and a particle-based Monte Carlo Localization.
We show that both implementations allow accurate localization on a large variety of patterns, from pseudo-random black and white matrices to grayscale drawings by children.
We discuss the optimal parameters and the real-time applicability of our algorithms.
Our solution to the localization of inexpensive mobile robots enables a new dimension of educational activites, strongly increasing the value of robots for education.
\end{abstract}

\section{Introduction}

Driven by consumer products, electronics, motor and battery technologies have made tremendous progress in the last decades.
They are now widely available at prices making affordable mobile robots a reality.
This opens many new opportunities, in particular in the educational context.
Robots have been shown to be effective in supporting the teaching of knowledge in fields such as mathematics, physics, computer science and engineering; and of skills such as problem solving and the scientific method~\cite{benitti2012explorin}.

Programmable platforms such as the LEGO Mindstorms, in the 300--400\,\$ price range, have been successfully used in classrooms worldwide, especially to teach engineering.
Yet, despite its high price tag, this platform has little perception capabilities; its longest-range sensor being a single ultrasound unit.
Cheaper products such as the Thymio~II (120\,\$) and Dash \& Dot (160\,\$) even lack such a long-range perception capability, and only provide local infrared sensors.
These are limited to sensing distances up to 15\,cm and measuring the grayscale intensity of a close object or the ground.
These products also lack a camera, because processing images would require extended memory and processing resources, which would increase their price significantly.
Therefore, all affordable educational robots (in the 100\,\$ range) lack long-range sensing capabilities.
In addition, dead-reckoning on these platforms is bad to non-existent, as proper wheel encoders are still too expensive on platforms that must already provide a variety of programming and sensing modalities to meet educational needs.
When dead-reckoning is provided, such as on the Thymio~II, it builds on approximate methods such as measuring the back electromotive force.
% A notable exception is the BeeBot, which uses proper dead-reckoning as its only sensor.
% But this robot can only follow pre-programmed trajectories, it is not able to react to external events.

The combined lack of long-range sensors and quality dead-reckoning prevents these robots from using state of the art localization algorithms, such as simultaneous localization and mapping.
This lack of absolute positioning limits their educational use, in particular for the study of geometry, physics, and engineering; but also the building of advanced behaviors, such as robots playing a theater piece.
% TODO ???robots playing a theater piece??? that's not a typical robot behaviour
\begin{figure}
\includegraphics[width=.80\columnwidth]{thymio_on_map}
\caption{Thymio on map....need further explanation}
\label{fig:thymio_on_map}
\end{figure}

This paper tackles this issue by providing an absolute positioning system using only approximate dead-reckoning and inexpensive infrared sensors measuring the grayscale intensity of the ground.
Our solution is based on the classical Markov and Monte Carlo Localization frameworks that can be seen as respectively a dense and sampling-based implementation of a recursive Bayesian filter.
%Our solution builds on a recursive Bayesian filter, of which we propose both a dense and a sampling-based implementation.
We evaluate these using the Thymio~II mobile robot on a known black and white pseudo-random pattern, with ground truth provided by a Vicon tracking system.
In addition, we provide a theoretical contribution predicting the traveled distance necessary to localize the robot.
Finally, we show how grayscale images can be used as ground maps that allow the robot to localize.

\section{Related work}

The challenge of solving localization problem of cheap mobile robots is the sensor information is very limited.
% Here need to add some opening sentence like we are doing cheap robots not very well equipped robots, bla bla bla.... One sentence is enough

Some approaches are developed to solve localization problem by using multiple robots instead of one.
In this case, collaborative sensors information is used instead of one.
Prorok et al.~\cite{prorok2012low} have used an infrared-based range and bearing hardware along with a distribute Monte Carlo approach, allowing a group of robots to localize down to a precision of 20\,cm.
However, this work only focuses on scalability rather than absolute positioning.
% Maybe one example is not enough?


% In opposition to this line of research, our work focus on absolute positioning of a single robot.
% Nevertheless, our approach is also fully distributed and therefore scales perfectly.

Solving localization problem with only one cheap robot with limited sensor information is even more challenging.
% The first sentence of this paragraph is not strong enough. Need to be polished....
Dias and Ventura~\cite{dias2013absolute} use barcode on the wall of an arena and apply an Extended Kalman Filter algorithm to localize an e-puck robot.
Their system reaches precision in the order of cm and 5°, but requires wall around the arena to place the bar codes.
Gutmann et al.~\cite{gutmann2013challenges} design a low-cost indoor localization system using 3-diode and 4-diode sensors. 
The average accuracy is from 7 to 12 cm.
However, this method request installation of projector to project infrared light spots on the ceiling.
Zug et al.~\cite{zug2011design} have developed an algorithm using an array of triangulation-based infrared distance sensors.
A Kalman filter algorithm is applied to localize the robot within a 2 by 1 meter box.  
However, experimental result is not provided. 
The simulation show the estimated error to be within 2\, cm. 
In \cite{pinto2012localization,singh2013map}, approaches are presented for solving localization problem with LEGO NXT.
Pinto et al.~\cite{pinto2012localization} use an Extended Kalman Filter algorithm is conducted for the purpose of teaching undergraduate university students localization of mobile robots.
The localization environment is with several white walls and EKF uses information from infra-red sensors equipped on the robot.
Singh and Bedi~\cite{singh2013map} use a particle filter algorithm is used to solve robot localization problem with robot kidnapping using information from ultrasonic sensors.
However, neither of these two works give experimental results, and they both depend on a very specific wall arrangement.
% Also I haven't added the example from Francesco... I don't know where to add, maybe before Gutmann?
% Some summary need to be added...
% While positioning in affordable mobile robots has been done before, no existing solution is as cheap and precise as the proposed system.


\section{Material}
\begin{figure}
\includegraphics[width=.56\columnwidth]{thymio2-shadow}\hfill
\includegraphics[width=.42\columnwidth]{thymio2-dimensions}
\caption{The Thymio~II robot and the placement of its ground sensors.}
\label{fig:thymio}
\end{figure}

We run empirical experiments with the Thymio~II mobile robot localizing on a printed map.

Thymio~II is an open-source differential-wheeled mobile robot (\fig{thymio}, left).
Its footprint is approximately $10 \times 10$ cm and it costs about 120\,\$.
Thousands of units have been sold to date and are used in the context of computational thinking and engineering education~\cite{riedo2015thymio}.
It features many sensors such as a set of infrared sensors, a small accelerometer, or a microphone.
This project uses only its two ground infrared sensors (\fig{thymio}, right) and the measurement of wheels speed.
Thymio~II is programmed through the Aseba framework~\cite{aseba2011tmech}, which, in this work, connects to the \textsc{ros} framework\footnote{ROS (Robot Operating System): \url{http://www.ros.org/}}.

The map consists in a 150$\times$150\,cm pattern containing 50\,$\times$\,50 black and white random cells of 3$\times$3\,cm.
The pattern is generated randomly with an equal probability of black and white cells and without regularization.
% TODO: what do you call regularization? no correlation between adjacent cells? can you clarify?

To evaluate the performance of our localization algorithms, we record the ground-truth pose of the robot using a Vicon\footnote{\url{http://www.vicon.com/}} tracking system.
%to provide the ground-truth robot pose, consisting of $x$, $y$ coordinates and the heading $\theta$.
We use \textsc{ros} to synchronously record these along with sensor values and odometry information from Thymio.
During the dataset gathering, the robot moves on the map with a speed of 3--5\,cm/s.
The data are gathered with a high enough frequency of 100\,Hz (ground-truth) and 10\,Hz (robot) and subsequently down-sampled to a period of 0.3\,s.
To fully explore the performance of the algorithm, we remotely control the robot along several trajectories covering all possible motions.
An example trajectory is shown in \fig{dataset}.
% TODO is this figure really necessary?

\begin{figure}
\begin{center}
\includegraphics[width=0.4\textwidth]{dataset}
\caption{An example trajectory of the robot. The robot moves on the map with random black and white pattern with three different motions: going forward, going backward and rotating on spot.}
\label{fig:dataset}
\end{center}
\end{figure}

The motions of the robot can be classified into two different types, namely moving forward or backward (possibly in arcs) and rotating on spot.
To test the algorithm's ability of recovering from catastrophic localization failures, we also conduct experiments with ``robot kidnapping'' by lifting the robot and moving it to another part of the map.

\section{Model}

\subsection{Variables}

The model uses the following variables:
\begin{itemize}
\item $X_{1:t}$ 2-D pose at times $1..t$.
The pose vector consists of $x,y$ coordinates and an angle $\theta$.
\item $Z_{1:t}$ observations at times $1..t$.
The observation consists of the output of two sensors located at the bottom of the robot, measuring the grayscale intensity of the ground.
\item $U_{1:t}$ odometry at times $1..t$.
The odometry consists of the left and right wheel speeds.
\end{itemize}

\subsection{Joint probability}

We formulate the model as a recursive Bayesian filter.
Its joint probability is:
\begin{equation*}
\begin{split}
& p(X_{1:t}, Z_{1:t}, U_{1:t}) = \\
& p(Z_t|X_t) p(X_t|X_{t-1}, U_{t}) p(U_t) p(X_{1:t-1}, Z_{1:t-1}, U_{1:t-1})
\end{split}
\end{equation*}

\subsection{Question}

We want to estimate the pose $X_t$ at time $t$ knowing the observations $Z_{1:t}$ and the commands $U_{1:t}$:
\begin{equation*}
\begin{split}
& p(X_t|Z_{1:t},U_{1:t}) = \frac{p(X_t,Z_t | Z_{1:t-1}, U_{1:t})}{p(Z_t|Z_{1:t-1}, U_{1:t})} \\
% &\propto p(Z_t | X_t) p(X_t | Z_{1:t-1}, U_{1:t}) \\
% &\propto p(Z_t | X_t) \sum_{X_{t-1}} p(X_t, X_{t-1} | Z_{1:t-1}, U_{1:t} ) \\
 &\propto p(Z_t | X_t) \sum_{X_{t-1}} p(X_t|X_{t-1}, U_t) p(X_{t-1} | Z_{1:t-1}, U_{1:t-1})
\end{split}
\end{equation*}
%This is a recursive filter to estimate $X_t$ using the previous estimation $X_{t-1}$, the odometry $U_t$ and the observation $Z_t$.

\subsection{Distributions}

There are two distributions to specify, the \emph{observation model} $p(Z_t | X_t)$ and the \emph{motion model} $p(X_t|X_{t-1}, U_t)$.

\subsubsection{Observation model}

The robot has two infrared ground sensors measuring the grayscale value of the ground.
Our observation model assumes these to be independent, so $p(Z_t | X_t) = \prod_{i=0,1} p(Z_t^{i} | X_t)$.
Moreover, we assume the ground color to be in the range $[0,1]$ with $0$ being black and $1$ being white.
Hence, if sensor $i$ for robot pose $X_t$ should see a ground intensity $v$ according to the map, $p(Z_t^{i} | X_t) \sim \mathcal{N}(v,\sigma_\mathrm{obs})$.
The parameter $\sigma_\mathrm{obs}$ is selected based on the knowledge of the sensor.
For black and white maps, we set it to $0.5$, giving a probability of $p_\mathrm{correct} = 0.95$ when the value of the sensor matches the one of the map.
For grayscale images, based on measurement on a real Thymio, we set it to $0.1$.

\subsubsection{Motion model}

Based on the model of Eliazar et al.~\cite{eliazar2004motionmodel}, we assume that the motion has a Gaussian error model, hence $p(X_t~|~X_{t-1}, U_{t})\sim\mathcal{N}(\mu_t,\Sigma_t)$.
The mean $\mu_t$ is built by accumulating the estimated displacements by dead-reckoning between times $t-1$ and $t$.
Therefore, if $\Delta x_t$, $\Delta y_t$, $\Delta \theta_t$ are the displacement between $t-1$ and $t$, expressed in the robot local frame at $t-1$, $\mu_t$ is:
\begin{equation*}
\mu_t =
\left[ \begin{array}{c} x_t \\ y_t \\ \theta_t \end{array} \right]
\text{with}
\begin{array}{c}
\left[ \begin{array}{c} x_t \\ y_t \end{array} \right] =
\left[ \begin{array}{c} x_{t-1} \\ y_{t-1} \end{array} \right] +
R(\theta_{t-1})
\left[ \begin{array}{c} \Delta x_{t} \\ \Delta y_{t} \end{array} \right]
\\
\theta_t = \theta_{t-1} + \Delta \theta_t
\end{array}
\end{equation*}
where $R(\theta)$ is the 2-D rotation matrix of angle $\theta$.
The $3\times3$ diagonal covariance matrix $\Sigma_t$ is a function of the distance traveled and the amount of rotation:
\begin{equation*}
\Sigma_t=\begin{bmatrix} \sigma_{\mathrm{xy},t}^2 & 0 & 0 \\ 0 & \sigma_{\mathrm{xy},t}^2 & 0 \\ 0 & 0 & (\alpha_\theta | \Delta \theta_t |)^2 \end{bmatrix}
\end{equation*}
with $ \sigma_{\mathrm{xy},t} = \alpha_\mathrm{xy} \sqrt{\Delta x_{t}^2 + \Delta y_{t}^2}$.

In addition, to cope for the possibility of the robot being kidnapped and therefore its pose becoming unknown, a uniform distribution with a weight $p_\mathrm{uniform}$ is added to $X_t$.

The parameters $\alpha_\mathrm{xy}$, $\alpha_\theta$ and $p_\mathrm{uniform}$ are found using maximum likelihood estimation (see \sect{mle}).

\subsection{Implementations}

We provide two implementations of the model, both programmed in Python, with Cython\footnote{\url{http://www.cython.org}} used for time-critical inner loops, leading to overall performance similar to C code.
The first implementation does Markov Localization using conditional probability tables to represent distributions~\cite{fox1999markov}.
The $x,y$ cell resolution is 1\,cm and the angular resolution varies from 20° (18 discretization steps) to 5° (72 discretization steps).
The second implementation does Monte Carlo Localization using a particle filter~\cite{dellaert1999monte}.
The pose is estimated by averaging around a particle that has many neighbors, using \textsc{ransac}~\cite{Fischler1981ransac}.
Both the search for the particle with most neighbors and the search for neighbors are done with 500 trials.
The thresholds for a particle to be a neighbor are a distance of 1.5\,cm and an angular difference of 5°.

\subsection{Theoretical analysis of convergence}
\label{sec:theoreticalconv}

% TODO text could be adapted, some equations removed, etc. in case it's too long.
It is possible to estimate the time required for the robot to localize itself in a given known space.
For the Markov Localization approach, we can see that there is a given number of discrete cells.
The amount of information needed to unambiguously specify one among them all is:
\begin{displaymath}
	H_\mathrm{loc} = \log_2(N_\mathrm{cells}) = \log_2\left(\frac{L\times W\times N_{\theta}}{h^2}\right),
\end{displaymath}
with $N_\mathrm{cells}$ the number of cells, $L$ and $W$ the length and width of the environment, $h$ the size of the cell, and $N_{\theta}$ the number of discretization steps of the angle $\theta$ of the robot.
In our example with a 150\,cm$\times$150\,cm environment discretized with cells of 1\,cm and 5° angle, the amount of information needed for the localization is around 20.6\,bit.

A binary sensor ideally yields 1\,bit of information per measurement.
However in practice, there is a loss in information due to the sensor noise, characterized by the $p_\mathrm{correct}$ probability of the sensor to be correct:
\begin{displaymath}
	H_\mathrm{noise} = H_{\text{b}}(1 - p_\mathrm{correct}),
\end{displaymath}
where $H_{\text{b}}$ is the binary entropy function: $H_{\text{b}}(p) = -p\log_2(p) - (1-p)\log_2(1-p)$.
With $p_\mathrm{correct}=0.95$ the loss in information is around 0.29\,bit per measurement.

In addition to the noise, we need to take into account that our sensor measurements are not completely independent.
For example, when not moving, we observe always the same place and thus cannot really gain additional information besides being sure of the color of the current pixel.
In a discretized world, we thus need to estimate the probability of having changed cell in order to observe something new, which depends on the distance traveled and the size of the cells.
This problem is equivalent to the Buffon-Laplace needle problem of finding the probability for a needle thrown randomly on a grid to actually intersect the grid~\cite{laplace1820prob}.
In our case, the probability of not changing cell is given by:
\begin{displaymath}
	p_\mathrm{same} = \frac{4d h - d^2}{\pi h^2},
\end{displaymath}
with $d$ the distance traveled.

We can then compute the conditional entropy for two successive ideal measurements separated by $d$:
\begin{displaymath}
	H(O_t | O_{t-1}) = H(O_t, O_{t-1}) - H(O_{t-1}).
	% That's a really generic formula, but giving the 2x2 matrix or the expression with the logs is probably too much
\end{displaymath}
With a robot moving at around 3\,cm/s with a timestep of 0.3\,s with 3\,cm cells, we have $d=0.9$\,cm and the loss of information due to the redundancy of around 0.33\,bit.

The computation is similar with a second sensor placed on the robot.
The probability that they see the same cell based on the distance between them is exactly the same.
On our small robots, the sensors are 2.2\,cm apart so their redundancy causes a loss of information of 0.041\,bit.

If we combine all these effects, we can bound the information that our robot gathers at each timestep to at most 1.05\,bit and a time to localize of at least 6\,s, which corresponds to a distance traveled of at least 18\,cm.
In order to have a faster localization, the greatest changes could be to move faster, in order to reduce redundancy in the successive measurements, or to have better sensors. % FIXME: SM: what do you mean?
Setting the sensors apart also reduces the redundancy between their information but given our grid size, they are sufficiently separated.

\section{Experiments}

\subsection{Dataset collection}

Our experiments are based on four datasets:
\begin{itemize}
\item \texttt{trajectory~1}: The robot goes forward, then backward, rotates on spot, and again forward and backward;
\item \texttt{trajectory~2}: The robot goes forward, turns on spot for a long duration, then goes forward and backward;
\item \texttt{trajectory with kidnapping}: The robot alternates phases of forward movement and turning on spot, with kidnapping happening every minute;
\item \texttt{linear trajectory}: The robot simply goes straight along the x-axis of the map.
\end{itemize}

\subsection{Parameter estimation}
\label{sec:mle}

The noise parameters of the motion model were estimated using maximum likelihood, considering the error between the ground truth and the odometry data in the local frame between two time steps.
Using \texttt{trajectory~1} and \texttt{trajectory~2}, the values for  $\alpha_\mathrm{xy}$ and $\alpha_\theta$ were found be in the order of 0.1.
Similarly, using \texttt{trajectory with kidnapping}, the value for $p_\mathrm{uniform}$ was also found to be in the order of 0.1.

\subsection{Basic localization}

\begin{figure*}

\begin{center}
Markov Localization, \texttt{trajectory~1}, for different \emph{number of discretization angles}
\end{center}
\includegraphics{ml-whole_random_1-xy}\hfill
\includegraphics{ml-whole_random_1-theta}

\vspace{.5em}

\begin{center}
Markov Localization, \texttt{trajectory~2}, for different \emph{number of discretization angles}
\end{center}
\includegraphics{ml-whole_random_2-xy}\hfill
\includegraphics{ml-whole_random_2-theta}

\vspace{.5em}

\begin{center}
Monte Carlo Localization, \texttt{trajectory~1}, for different \emph{number of particles}
\end{center}
\includegraphics{mcl-whole_random_1-xy}\hfill
\includegraphics{mcl-whole_random_1-theta}

\vspace{.5em}

\begin{center}
Monte Carlo Localization, \texttt{trajectory~2}, for different \emph{number of particles}
\end{center}
\includegraphics{mcl-whole_random_2-xy}\hfill
\includegraphics{mcl-whole_random_2-theta}

\caption{The error between the estimation by the localization algorithm and the ground truth on \texttt{trajectory~1} (top) and \texttt{trajectory~2} (bottom).
For Monte Carlo Localization, the solid lines show the average over 10 trials, while the light dots show the individual trials.}
\label{fig:whole-runs-random12}
\end{figure*}

\Fig{whole-runs-random12} shows the error in position and orientation for the first two trajectories.
In these plots, \emph{distance traveled} represents the cumulative distance traveled by the center point between the two ground sensors of the robot.
The error is clamped at 50\,cm for position and 90° for orientation.

For Markov Localization approach, all discretization resolutions allow the robot to localize with a precision of 3\,cm and 5°.
In \texttt{trajectory~1}, the resolution of 18 is not enough to keep tracking the orientation at a distance traveled of 50\,cm and 80\,cm.
These both correspond to the robot rotating on spot.
We see that an angular discretization of 36 (10° resolution) is sufficient to provide accurate tracking, and that a finer resolution only provides minor improvements in angular precision, and no improvement in position precision.
In \texttt{trajectory~2}, we see that a resolution of 54 allows for a better angular precision than 36, but 72 does not improve over 54.
All resolutions provide equal position precision.

For Monte Carlo Localization approach, we see that on \texttt{trajectory~1}, the robot localizes already with 50k particles, but twice later than with 100k particles.
Increasing the number of particles over this value only provides minor decrease in localization time.
While 50k particles allow to localize on this run in average, there are some trials (restart of the run) in which the robot looses orientation, when it turns on spot.
On \texttt{trajectory~2}, 50k particles is not enough for localizing the robot.
Increasing this number to 100k leads to a good localization, excepted after the robot has traveled 80\,cm; which corresponds to a long moment during which the robot rotates on spot, leading to less information acquisition, and therefore degraded precision.

Overall, both approaches have similar localization accuracy.
When angular precision is critical, the Monte Carlo Localization approach might achieve better performance, as the Markov Localization approach is limited in precision by its discretization.

%TODO: explain why this does not appear in ML

\subsection{Distance to converge}

\begin{figure}

\begin{center}
Markov Localization, 10 segments from the first two trajectories, for different \emph{number of discretization angles}
\end{center}
\includegraphics{ml-small_runs_random_12-xy}

\vspace{.5em}

\begin{center}
Monte Carlo Localization, 10 segments from the first two trajectories, for different \emph{number of particles}
\end{center}
\includegraphics{mcl-small_runs_random_12-xy}

\caption{
The error between the estimation by the localization algorithm and the ground truth on 10 segments from the first two trajectories.
The solid lines show the median while the light dots show the individual trials.}
\label{fig:small-runs}
\end{figure}

\Fig{small-runs} shows the error in position for 5 different locations in the two first trajectories.
We see that, with Markov Localization approach, the correct pose is found after about 20\,cm.
This corresponds well to the ideal theoretical distance of 18\,cm computed in \sect{theoreticalconv}.
However, there are outliers.
This happens when the robot is turning on spot, in which case there is not enough information to localize the robot.
The convergence is slower with Monte Carlo Localization approach, except with 400k particles, in which case it is roughly similar.
Decreasing the number of particles quickly increases the distance to converge, reaching 60\,cm for 100k particles.
Using only 50k particles, some trajectory segments fail to converge, even after 80\,cm length.

\Fig{small-maps} shows the effect on convergence distance when the map size is reduced.
The robot runs linearly on one quarter of the map, while the Markov Localization is provided with the whole map, half of it, and a quarter of it.
We see that reducing the map size does reduce the distance traveled necessary to converge, in accordance to the theory.

\begin{figure}
\includegraphics{ml-small_maps-xy}
%\vspace{.5em}
%\includegraphics{ml-small_maps-theta}
\caption{The error between the estimation by the localization algorithm and the ground truth, using Markov Localization, for different map sizes on \texttt{linear trajectory}.
The solid lines show the median over 3 different sizes, while the light dots show the individual sizes.}
\label{fig:small-maps}
\end{figure}

\subsection{Robot kidnapping}

\begin{figure}

\begin{center}
Markov Localization, \texttt{trajectory with kidnapping}, for different \emph{number of discretization angles}
\end{center}
\includegraphics{ml-whole_random_long-xy}

\vspace{.5em}

\begin{center}
Monte Carlo Localization, \texttt{trajectory with kidnapping}, for different \emph{number of particles}
\end{center}
\includegraphics{mcl-whole_random_long-xy}

\caption{The error between the estimation by the localization algorithm and the ground truth on the run with kidnapping.
For Monte Carlo Localization, the solid lines show the median over 10 trials, while the light dots show the individual trials.}
\label{fig:whole-runs-random-long}
\end{figure}

\Fig{whole-runs-random-long} shows the error in position for the run with kidnapping.
In this run, the robot is kidnapped twice, after having traveled 550\,cm and 1000\,cm.
It takes the robot approximately 50\,cm to re-localize, and does so successfully with both Markov and Monte Carlo Localization approaches.
% FIXME: check on the video for numbers, edit plots
With Markov Localization approach, discretization resolutions of 36, 54 and 72 are approximately equivalent in performance, while 18 performs clearly worst, but only for angular resolution.
With Monte Carlo Localization approach, the robot localizes most of the time with 100k particle or more, and always with 200k particles or more.
With 50k particles, the robot eventually localizes, but this might take more than 2 meters of traveled distance.
%TODO: reanalyse MCL once we have 10 runs
%FIXME: make more clear

\subsection{Computational cost}

\begin{figure}
\includegraphics{cpu_load}
\caption{The execution duration, proportional to the computational cost, for one time step.
For Markov Localization, the legend indicates the number of discretization steps for angles; for Monte Carlo Localization, the number of particles.
Data are averaged over all time steps of first two trajectories.}
\label{fig:cpuload}
\end{figure}

\Fig{cpuload} shows the duration of one step for the different algorithms and parameters, averaged over the two first trajectories.
These data were measured on a Lenovo laptop T450s with an Intel Core i7 5600U processor running at 2.6\,GHz.
We see that with the Markov Localization approach, the duration scales linearly with the number of discretization angles.
With the Monte Carlo Localization approach, the scaling is linear but amortized (50k particles is not twice faster as 100k).
This is due to the selection of pose estimate, which uses a \textsc{ransac} approach and is therefore independent of the number of particles.
For similar localization accuracy, the Monte Carlo Localization approach is slower than the Markov Localization approach, and therefore we suggest to use the former with a discretization angle of 36 in practical applications.
However, the Monte Carlo Localization approach might be preferred if a high angular precision is required.

At a computation duration of 1.5\,s per step, the Monte Carlo Localization approach with a discretization angle of 36 is far from real time.
However, this computation duration corresponding to a map size of 150\,cm.
If the map was smaller, for instance 50\,cm, the algorithm would be 9 times faster.
Therefore, at 150\,ms per step, it would be suitable for real-time operations.
Moreover, the code could be further optimized by using multimedia instructions such as \textsc{sse}, implementing it in the \textsc{gpu}, or parallelizing it.
Conditional probability tables are well suited for such optimizations.
The Monte Carlo Localization approach requires at least 100k particle for proper localization, at a computation duration of 2\,s per step.
While similar optimization is possible, some parts are harder to optimize, in particular the re-sampling step and the \textsc{ransac} step.

\subsection{Maps as grayscale images}

\begin{figure*}
\includegraphics[width=.15\textwidth]{maps/breugel_babel} \hfill
\includegraphics[width=.15\textwidth]{maps/van-gogh_starry-night} \hfill
\includegraphics[width=.15\textwidth]{maps/kandinsky_comp-8} \hfill
\includegraphics[width=.15\textwidth]{maps/vermeer_girl-pearl} \hfill
\includegraphics[width=.15\textwidth]{maps/babar} \hfill
\includegraphics[width=.15\textwidth]{maps/childs-drawing_tooth-fairy}

\makebox[.15\textwidth][c]{Breugel}\hfill
\makebox[.15\textwidth][c]{Van Gogh}\hfill
\makebox[.15\textwidth][c]{Kandinsky}\hfill
\makebox[.15\textwidth][c]{Vermeer}\hfill
\makebox[.15\textwidth][c]{Babar}\hfill
\makebox[.15\textwidth][c]{Child's drawing}

\begin{center}
\texttt{trajectory~1}
\end{center}
\includegraphics{ml-grayscale_images-random_1-xy}\hfill
\includegraphics{ml-grayscale_images-random_1-theta}

\vspace{.5em}

\begin{center}
\texttt{trajectory~2}
\end{center}
\includegraphics{ml-grayscale_images-random_2-xy}\hfill
\includegraphics{ml-grayscale_images-random_2-theta}

\caption{The error between the estimation by the localization algorithm and the ground truth, using Markov Localization with 36 angle discretization, for different grayscale maps.}
\label{fig:grayscale}
\end{figure*}

In addition to random black and white patterns, our localization model can also work on grayscale images.
While we have not recorded datasets with such images, we can simulate these scenarios by using the ground-truth data to look up the corresponding pixel on the image, adding some noise ($\pm$ 0.1, uniform distribution, found using a real Thymio).
When running on such maps, we set $\sigma_\mathrm{obs}$ to 0.1.

\Fig{grayscale} shows the error in position and orientation for six different grayscale maps.
These cover various styles, contrast profiles and distributions of intensities.
The first three, paintings from Breugel, Van Gogh and Kandinsky, allow the robot to localize in less than 10\,cm for both \texttt{trajectory~1} and \texttt{trajectory~2}.
This is due to their variety of contrasted visual structures.
In areas with less contrast, the angular tracking becomes imprecise, but is always within 20°.
The Vermeer painting is harder.
While the robot localizes within 20\,cm on \texttt{trajectory~1}, it has trouble with \texttt{trajectory~2}, especially with the angle.
We attribute this degraded performance to the large areas without contrast in this painting.
The next image is a line-art drawing from a popular children comics.
Stable localization on \texttt{trajectory~1} is only achieved after traveling 150\,cm.
The reason is the low amount of information present in such images.
Nevertheless, when the trajectory encounters sufficient information, like in \texttt{trajectory~2}, accurate localization is possible.
The last image is a child's drawing taken from Wikimedia Commons\footnote{\url{https://commons.wikimedia.org/wiki/File:Child's_Drawing_of_the_Tooth_Fairy.jpg}}.
This drawing allows accurate localization, after traveling 80\,cm in the first run, but only 20\,cm in the second.
This shows that with our model, the robot can localize successfully on maps drawn by children themselves, which opens creative educational opportunities.

\section{Conclusion}

In this paper, we have conducted an empirical evaluation of Markov- and Monte Carlo-based approaches for localizing a mobile robot, equipped with two inexpensive ground sensors, on a known map.
We have shown that both approaches allow successful localization, and that their performances and computational requirements are of a similar order of magnitude.
With current implementations, Markov localization with 36 discretization steps for angle is the most promising choice for deployment in educational activities.
In addition, we have outlined, and empirically validated, a theoretical analysis method to estimate the localization performance in function of the sensor configuration.
This method provides a guide for taking decisions about the placement of sensors in a future robot:
localization performance can be improved by placing the sensors far apart on a line perpendicular to the direction of movement of the robot; in addition, more sensors allow to collect more information, if they are separated by the size of the smallest visual structure in the map.
Finally, we have demonstrated that our model allows successful localization on a large variety of grayscale images, from well-known paintings to drawings made by children.
This opens the way to user-created maps, for instance for a child making a picture with her mobile phone of her own drawing, printing this picture on a poster, and using it as map.

These contributions to the state of the art enable absolute positioning of inexpensive mobile robots costing less than 100\,\$.
In the context of educational robotics, this opens many opportunities, such as the study of geometry, puzzles based on the physical space, and robots playing a theater piece, for instance in the context of language education.
By creating a new dimension of educational activites, our solution also strongly increases the value of robots for education.
%These new possibilities are key elements for robots to answer the need for embodied computational thinking education.
% TODO: finish on constructionism?

\section{Acknowledgements}

The authors thank Emmanuel Eckard for insightful comments on the manuscript.

\bibliographystyle{IEEEtran}
\bibliography{thymio-localisation-icra}

\end{document}



